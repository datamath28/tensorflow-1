{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pandas import DataFrame\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (20000, 28, 28) (20000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (18724, 28, 28) (18724,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print 'Training set', train_dataset.shape, train_labels.shape\n",
    "  print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "  print 'Test set', test_dataset.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (20000, 784) (20000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (18724, 784) (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print 'Training set', train_dataset.shape, train_labels.shape\n",
    "print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "print 'Test set', test_dataset.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compue the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = 1024\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "  # Variables.\n",
    "  weights_hidden = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "\n",
    "  biases_hidden = tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([hidden_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  def inference(data):\n",
    "    hidden_relu = tf.nn.relu(tf.matmul(data, weights_hidden) + biases_hidden)\n",
    "    logits = tf.matmul(hidden_relu, weights) + biases\n",
    "    return logits\n",
    "\n",
    "  train_logits = inference(tf_train_dataset)\n",
    "    \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(train_logits)\n",
    "  valid_prediction = tf.nn.softmax(inference(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(inference(tf_test_dataset))\n",
    "\n",
    "  cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(train_logits, tf_train_labels))\n",
    "  l2_loss = tf.nn.l2_loss(weights_hidden) + tf.nn.l2_loss(weights)\n",
    "  \n",
    "  loss = cross_entropy + l2_loss * 0.001\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 628.956\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 30.1%\n",
      "Minibatch loss at step 500 : 190.001\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1000 : 118.025\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 1500 : 69.3027\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2000 : 42.0285\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2500 : 25.533\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 3000 : 15.5622\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.8%\n",
      "Test accuracy: 91.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print \"Initialized\"\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print \"Minibatch loss at step\", step, \":\", l\n",
    "      print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "      print \"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels)\n",
    "  print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 1024\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "  # Variables.\n",
    "  weights_hidden = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "\n",
    "  biases_hidden = tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([hidden_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  def inference(data, keep_proba=1):\n",
    "    dropout = tf.nn.dropout(tf.matmul(data, weights_hidden) + biases_hidden, keep_proba)\n",
    "    hidden_relu = tf.nn.relu(dropout)\n",
    "    logits = tf.matmul(hidden_relu, weights) + biases\n",
    "    return logits\n",
    "\n",
    "  train_logits = inference(tf_train_dataset, keep_proba=0.5)\n",
    "    \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(train_logits)\n",
    "  valid_prediction = tf.nn.softmax(inference(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(inference(tf_test_dataset))\n",
    "\n",
    "  cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(train_logits, tf_train_labels))\n",
    "  l2_loss = tf.nn.l2_loss(weights_hidden) + tf.nn.l2_loss(weights)\n",
    "  \n",
    "  loss = cross_entropy + l2_loss * 0.001\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 800.766\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 27.4%\n",
      "Minibatch loss at step 500 : 201.008\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1000 : 118.267\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 1500 : 69.5161\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 2000 : 42.2701\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2500 : 25.662\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 3000 : 15.7503\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.3%\n",
      "Test accuracy: 91.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print \"Initialized\"\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print \"Minibatch loss at step\", step, \":\", l\n",
    "      print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "      print \"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels)\n",
    "  print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = DataFrame(columns=('hidden_size', \n",
    "                        'batch_size', \n",
    "                        'layer_1_keep_proba', \n",
    "                        'layer_2_keep_proba',\n",
    "                        'weights_1_penalty',\n",
    "                        'weights_2_penalty',\n",
    "                        'weights_penalty',\n",
    "                        'step_size',\n",
    "                        'num_steps',\n",
    "                        'test_accuracy',\n",
    "                        'slope'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['step_size_decay'] = [1] * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>layer_1_keep_proba</th>\n",
       "      <th>layer_2_keep_proba</th>\n",
       "      <th>weights_1_penalty</th>\n",
       "      <th>weights_2_penalty</th>\n",
       "      <th>weights_penalty</th>\n",
       "      <th>step_size</th>\n",
       "      <th>num_steps</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>slope</th>\n",
       "      <th>step_size_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>11</td>\n",
       "      <td>41.839351</td>\n",
       "      <td>2.297000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>3001</td>\n",
       "      <td>89.430677</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>3001</td>\n",
       "      <td>89.708396</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>3001</td>\n",
       "      <td>89.366588</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>3001</td>\n",
       "      <td>89.158300</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>5001</td>\n",
       "      <td>89.500107</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>5001</td>\n",
       "      <td>89.345225</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>5001</td>\n",
       "      <td>86.349071</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>5001</td>\n",
       "      <td>89.532151</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5001</td>\n",
       "      <td>89.334544</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>5001</td>\n",
       "      <td>89.419996</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>9001</td>\n",
       "      <td>89.590899</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>9001</td>\n",
       "      <td>89.692373</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>4001</td>\n",
       "      <td>89.131596</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>4001</td>\n",
       "      <td>88.987396</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>4001</td>\n",
       "      <td>88.784448</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>4001</td>\n",
       "      <td>89.099551</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>6001</td>\n",
       "      <td>89.323862</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>6001</td>\n",
       "      <td>89.035463</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>6001</td>\n",
       "      <td>89.398633</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>6001</td>\n",
       "      <td>89.692373</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>12001</td>\n",
       "      <td>89.911344</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>12001</td>\n",
       "      <td>90.493484</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>12001</td>\n",
       "      <td>90.659047</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>15001</td>\n",
       "      <td>90.488144</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>20001</td>\n",
       "      <td>89.809870</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>20001</td>\n",
       "      <td>90.322581</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>20001</td>\n",
       "      <td>90.440077</td>\n",
       "      <td>-0.000245</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hidden_size  batch_size  layer_1_keep_proba  layer_2_keep_proba  \\\n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.50                0.50   \n",
       "0         1024         128                0.50                0.50   \n",
       "0         1024         128                1.00                0.50   \n",
       "0          512         128                1.00                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.75   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.80                0.50   \n",
       "0         1024         128                0.80                0.50   \n",
       "0         1024         128                0.75                0.75   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.65                0.65   \n",
       "0         1024         128                0.60                0.60   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "\n",
       "   weights_1_penalty  weights_2_penalty  weights_penalty  step_size  \\\n",
       "0              0.050              0.005           0.0100     0.0005   \n",
       "0              0.050              0.005           0.0100     0.0005   \n",
       "0              0.050              0.005           0.0010     0.0005   \n",
       "0              0.010              0.005           0.0010     0.0005   \n",
       "0              0.010              0.005           0.0010     0.0005   \n",
       "0              0.005              0.005           0.0010     0.0005   \n",
       "0              0.050              0.005           0.0010     0.0005   \n",
       "0              0.050              0.005           0.0010     0.0005   \n",
       "0              0.100              0.005           0.0010     0.0005   \n",
       "0              0.100              0.005           0.0010     0.0002   \n",
       "0              0.030              0.005           0.0010     0.0002   \n",
       "0              0.030              0.005           0.0010     0.0002   \n",
       "0              0.050              0.005           0.0010     0.0002   \n",
       "0              0.050              0.005           0.0030     0.0002   \n",
       "0              0.050              0.005           0.0003     0.0002   \n",
       "0              0.050              0.001           0.0005     0.0002   \n",
       "0              0.050              0.010           0.0005     0.0002   \n",
       "0              0.050              0.005           0.0010     0.0002   \n",
       "0              0.050              0.005           0.0010     0.0002   \n",
       "0              0.050              0.005           0.0010     0.0004   \n",
       "0              0.050              0.005           0.0010     0.0005   \n",
       "0              0.050              0.005           0.0010     0.0005   \n",
       "0              0.050              0.005           0.0010     0.0005   \n",
       "0              0.050              0.005           0.0010     0.0005   \n",
       "0              0.050              0.005           0.0010     0.0005   \n",
       "0              0.010              0.005           0.0010     0.0005   \n",
       "0              0.020              0.005           0.0010     0.0005   \n",
       "0              0.050              0.050           0.0010     0.0005   \n",
       "\n",
       "   num_steps  test_accuracy     slope  step_size_decay  \n",
       "0         11      41.839351  2.297000                1  \n",
       "0       3001      89.430677  0.000330                1  \n",
       "0       3001      89.708396  0.000015                1  \n",
       "0       3001      89.366588  0.000270                1  \n",
       "0       3001      89.158300  0.000180                1  \n",
       "0       5001      89.500107  0.000015                1  \n",
       "0       5001      89.345225  0.000150                1  \n",
       "0       5001      86.349071 -0.000065                1  \n",
       "0       5001      89.532151  0.000090                1  \n",
       "0       5001      89.334544  0.000200                1  \n",
       "0       5001      89.419996  0.000315                1  \n",
       "0       9001      89.590899 -0.000040                1  \n",
       "0       9001      89.692373 -0.000030                1  \n",
       "0       4001      89.131596  0.000040                1  \n",
       "0       4001      88.987396  0.000125                1  \n",
       "0       4001      88.784448  0.000230                1  \n",
       "0       4001      89.099551  0.000265                1  \n",
       "0       6001      89.323862  0.000120                1  \n",
       "0       6001      89.035463  0.000170                1  \n",
       "0       6001      89.398633 -0.000030                1  \n",
       "0       6001      89.692373  0.000385                1  \n",
       "0      12001      89.911344 -0.000070                1  \n",
       "0      12001      90.493484  0.000295                1  \n",
       "0      12001      90.659047 -0.000035                1  \n",
       "0      15001      90.488144  0.000065                1  \n",
       "0      20001      89.809870 -0.000510                1  \n",
       "0      20001      90.322581 -0.000285                1  \n",
       "0      20001      90.440077 -0.000245                1  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = 1024\n",
    "batch_size = 128\n",
    "\n",
    "# dropout probabilities. Lower numbers mean more dropout and therefore more regularization\n",
    "layer_1_keep_proba = 0.5\n",
    "layer_2_keep_proba = 0.5\n",
    "#L2 penalty for weights. Higher numbers mean more penalty, and therefore more regularization\n",
    "weights_1_penalty = 0.025\n",
    "weights_2_penalty = 0.01\n",
    "weights_penalty = 0.02\n",
    "# Step size\n",
    "starting_step_size = 0.005\n",
    "step_size = starting_step_size\n",
    "step_size_decay = 0.9\n",
    "MOVING_AVERAGE_DECAY = 0.9995\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_size, hidden_size]))\n",
    "  biases2 = tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([hidden_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Track the moving averages of all trainable variables.\n",
    "  variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "  variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "  def getVar(v, averaged):\n",
    "        if averaged:\n",
    "            return variable_averages.average(v)\n",
    "        else:\n",
    "            return v\n",
    "  \n",
    "  # Training computation.\n",
    "  def inference(data, dropout=False, averaged=False):\n",
    "    # Dropout for training, no dropout for inference\n",
    "    if not dropout:\n",
    "        l1_dropout = 1\n",
    "        l2_dropout = 1\n",
    "    else:\n",
    "        l1_dropout = layer_1_keep_proba\n",
    "        l2_dropout = layer_2_keep_proba\n",
    "        \n",
    "    # Get the current version for training or the averaged version for \n",
    "    # evaluation\n",
    "    w1 = getVar(weights1, averaged)\n",
    "    b1 = getVar(biases1, averaged)\n",
    "    w2 = getVar(weights2, averaged)\n",
    "    b2 = getVar(biases2, averaged)\n",
    "    w = getVar(weights, averaged)\n",
    "    b = getVar(biases, averaged)\n",
    "    \n",
    "    # actual network archetecture\n",
    "    relu1 = tf.nn.relu(tf.matmul(data, w1) + b1)\n",
    "    dropout1 = tf.nn.dropout(relu1, l1_dropout)\n",
    "    relu2 = tf.nn.relu(tf.matmul(dropout1, w2) + b2)\n",
    "    dropout2 = tf.nn.dropout(relu2, l2_dropout)\n",
    "    logits = tf.matmul(dropout2, w) + b\n",
    "    return logits\n",
    "\n",
    "  train_logits = inference(tf_train_dataset, dropout=True)\n",
    "    \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(train_logits)\n",
    "  valid_prediction = tf.nn.softmax(inference(tf_valid_dataset, averaged=True))\n",
    "  test_prediction = tf.nn.softmax(inference(tf_test_dataset, averaged=True))\n",
    "\n",
    "  cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(train_logits, tf_train_labels))\n",
    "\n",
    "  l2_loss = (weights_1_penalty * tf.nn.l2_loss(weights1) \n",
    "             + weights_2_penalty * tf.nn.l2_loss(weights2)\n",
    "             + weights_penalty * tf.nn.l2_loss(weights))\n",
    "  \n",
    "  loss = cross_entropy + l2_loss\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(step_size).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 26360.6\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 8.7%\n",
      "Minibatch loss at step 200 : 12620.4\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 16.1%\n",
      "Minibatch loss at step 400 : 11394.7\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 26.3%\n",
      "Minibatch loss at step 600 : 10810.0\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 36.8%\n",
      "Minibatch loss at step 800 : 10335.6\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 51.3%\n",
      "Minibatch loss at step 1000 : 9894.1\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 62.3%\n",
      "Minibatch loss at step 1200 : 9442.6\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 1400 : 9007.43\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 1600 : 8660.94\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 1800 : 8326.05\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 2000 : 8019.69\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 2200 : 7725.22\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 2400 : 7410.15\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 2600 : 7129.3\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 2800 : 6885.69\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 3000 : 6628.1\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 3200 : 6386.47\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 3400 : 6157.79\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 3600 : 5942.47\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 3800 : 5733.51\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 4000 : 5524.56\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 4200 : 5333.06\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 4400 : 5148.94\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 4600 : 4967.83\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 4800 : 4798.57\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 5000 : 4638.2\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 89.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "validation_accuracies = []\n",
    "step_num = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print \"Initialized\"\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions, _ = session.run(\n",
    "      [optimizer, loss, train_prediction, variable_averages_op], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 200 == 0):\n",
    "      print \"Minibatch loss at step\", step, \":\", l\n",
    "      print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "      valid_accuracy = accuracy(\n",
    "        valid_prediction.eval(), valid_labels)\n",
    "      print \"Validation accuracy: %.1f%%\" % valid_accuracy\n",
    "      validation_accuracies.append(valid_accuracy)\n",
    "      step_num.append(step)\n",
    "      step_size *= step_size_decay\n",
    "    \n",
    "  test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "  print \"Test accuracy: %.1f%%\" % test_accuracy\n",
    "  run_info = dict()\n",
    "  run_info['hidden_size'] = hidden_size \n",
    "  run_info['batch_size'] = batch_size\n",
    "  run_info['layer_1_keep_proba'] = layer_1_keep_proba\n",
    "  run_info['layer_2_keep_proba'] = layer_2_keep_proba\n",
    "  run_info['weights_1_penalty'] = weights_1_penalty\n",
    "  run_info['weights_2_penalty'] = weights_2_penalty\n",
    "  run_info['weights_penalty'] = weights_penalty\n",
    "  run_info['step_size'] = starting_step_size\n",
    "  run_info['num_steps'] = num_steps\n",
    "  run_info['test_accuracy'] = test_accuracy\n",
    "  run_info['step_size_decay'] = step_size_decay\n",
    "  \n",
    "  X = np.array(step_num[-5:]).reshape((-1, 1))\n",
    "  y = np.array(validation_accuracies[-5:])\n",
    "  lr = LinearRegression()\n",
    "  lr.fit(X, y)\n",
    "  # Measures how much the error is continuing to decrease at the end of the run.\n",
    "  # values above 0 suggest longer training time.  \n",
    "  run_info['slope'] = lr.coef_[0]\n",
    "  df = df.append([run_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>layer_1_keep_proba</th>\n",
       "      <th>layer_2_keep_proba</th>\n",
       "      <th>weights_1_penalty</th>\n",
       "      <th>weights_2_penalty</th>\n",
       "      <th>weights_penalty</th>\n",
       "      <th>step_size</th>\n",
       "      <th>num_steps</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>slope</th>\n",
       "      <th>step_size_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>11</td>\n",
       "      <td>41.839351</td>\n",
       "      <td>2.297000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>10001</td>\n",
       "      <td>80.954924</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>5001</td>\n",
       "      <td>86.349071</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>4001</td>\n",
       "      <td>88.784448</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>4001</td>\n",
       "      <td>88.987396</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>6001</td>\n",
       "      <td>89.035463</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>4001</td>\n",
       "      <td>89.099551</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>4001</td>\n",
       "      <td>89.131596</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>3001</td>\n",
       "      <td>89.158300</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>8001</td>\n",
       "      <td>89.190344</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>6001</td>\n",
       "      <td>89.323862</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>5001</td>\n",
       "      <td>89.334544</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>5001</td>\n",
       "      <td>89.345225</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>3001</td>\n",
       "      <td>89.366588</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>6001</td>\n",
       "      <td>89.398633</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>5001</td>\n",
       "      <td>89.419996</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>3001</td>\n",
       "      <td>89.430677</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>5001</td>\n",
       "      <td>89.500107</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>5001</td>\n",
       "      <td>89.532151</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>10001</td>\n",
       "      <td>89.542833</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>9001</td>\n",
       "      <td>89.590899</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>6001</td>\n",
       "      <td>89.692373</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>9001</td>\n",
       "      <td>89.692373</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>3001</td>\n",
       "      <td>89.708396</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>3001</td>\n",
       "      <td>89.809870</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>20001</td>\n",
       "      <td>89.809870</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>12001</td>\n",
       "      <td>89.911344</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>8001</td>\n",
       "      <td>90.018159</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>10001</td>\n",
       "      <td>90.039521</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>3001</td>\n",
       "      <td>90.087588</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10001</td>\n",
       "      <td>90.092929</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>3001</td>\n",
       "      <td>90.098270</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4001</td>\n",
       "      <td>90.140996</td>\n",
       "      <td>-0.000605</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4001</td>\n",
       "      <td>90.140996</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10001</td>\n",
       "      <td>90.146336</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10001</td>\n",
       "      <td>90.221107</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>20001</td>\n",
       "      <td>90.322581</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4001</td>\n",
       "      <td>90.343944</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>20001</td>\n",
       "      <td>90.440077</td>\n",
       "      <td>-0.000245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>20001</td>\n",
       "      <td>90.482803</td>\n",
       "      <td>-0.000275</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>15001</td>\n",
       "      <td>90.488144</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>12001</td>\n",
       "      <td>90.493484</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>10001</td>\n",
       "      <td>90.509507</td>\n",
       "      <td>-0.000245</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>8001</td>\n",
       "      <td>90.610981</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>10001</td>\n",
       "      <td>90.637684</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>8001</td>\n",
       "      <td>90.643025</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>12001</td>\n",
       "      <td>90.659047</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>8001</td>\n",
       "      <td>90.739158</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>8001</td>\n",
       "      <td>90.765862</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hidden_size  batch_size  layer_1_keep_proba  layer_2_keep_proba  \\\n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.50                0.50   \n",
       "0          512         128                1.00                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.75   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.50                0.50   \n",
       "0         1024         128                0.50                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                1.00                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.50                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.80                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.90                0.50   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.60                0.60   \n",
       "0         1024         128                0.80                0.50   \n",
       "0         1024         128                0.60                0.60   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.60                0.60   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.75                0.65   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.65                0.65   \n",
       "0         1024         128                0.75                0.75   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "0         1024         128                0.70                0.70   \n",
       "\n",
       "   weights_1_penalty  weights_2_penalty  weights_penalty  step_size  \\\n",
       "0              0.050              0.005           0.0100   0.000500   \n",
       "0              0.010              0.010           0.0010   0.000046   \n",
       "0              0.050              0.005           0.0010   0.000500   \n",
       "0              0.050              0.001           0.0005   0.000200   \n",
       "0              0.050              0.005           0.0003   0.000200   \n",
       "0              0.050              0.005           0.0010   0.000200   \n",
       "0              0.050              0.010           0.0005   0.000200   \n",
       "0              0.050              0.005           0.0030   0.000200   \n",
       "0              0.010              0.005           0.0010   0.000500   \n",
       "0              0.050              0.010           0.0010   0.000067   \n",
       "0              0.050              0.005           0.0010   0.000200   \n",
       "0              0.100              0.005           0.0010   0.000200   \n",
       "0              0.050              0.005           0.0010   0.000500   \n",
       "0              0.010              0.005           0.0010   0.000500   \n",
       "0              0.050              0.005           0.0010   0.000400   \n",
       "0              0.030              0.005           0.0010   0.000200   \n",
       "0              0.050              0.005           0.0100   0.000500   \n",
       "0              0.005              0.005           0.0010   0.000500   \n",
       "0              0.100              0.005           0.0010   0.000500   \n",
       "0              0.010              0.020           0.0100   0.002500   \n",
       "0              0.030              0.005           0.0010   0.000200   \n",
       "0              0.050              0.005           0.0010   0.000500   \n",
       "0              0.050              0.005           0.0010   0.000200   \n",
       "0              0.050              0.005           0.0010   0.000500   \n",
       "0              0.050              0.020           0.0050   0.002500   \n",
       "0              0.010              0.005           0.0010   0.000500   \n",
       "0              0.050              0.005           0.0010   0.000500   \n",
       "0              0.050              0.010           0.0010   0.000244   \n",
       "0              0.050              0.050           0.0500   0.002500   \n",
       "0              0.050              0.020           0.0100   0.002500   \n",
       "0              0.050              0.020           0.0010   0.001000   \n",
       "0              0.010              0.020           0.0100   0.002500   \n",
       "0              0.050              0.025           0.0100   0.002500   \n",
       "0              0.050              0.030           0.0200   0.002500   \n",
       "0              0.050              0.050           0.0010   0.001000   \n",
       "0              0.050              0.020           0.0010   0.001000   \n",
       "0              0.020              0.005           0.0010   0.000500   \n",
       "0              0.050              0.020           0.0050   0.002500   \n",
       "0              0.050              0.050           0.0010   0.000500   \n",
       "0              0.050              0.010           0.0010   0.000006   \n",
       "0              0.050              0.005           0.0010   0.000500   \n",
       "0              0.050              0.005           0.0010   0.000500   \n",
       "0              0.050              0.010           0.0010   0.001000   \n",
       "0              0.050              0.020           0.0050   0.002500   \n",
       "0              0.050              0.020           0.0050   0.002500   \n",
       "0              0.050              0.005           0.0010   0.000033   \n",
       "0              0.050              0.005           0.0010   0.000500   \n",
       "0              0.050              0.020           0.0010   0.000033   \n",
       "0              0.050              0.020           0.0100   0.002500   \n",
       "\n",
       "   num_steps  test_accuracy     slope  step_size_decay  \n",
       "0         11      41.839351  2.297000              1.0  \n",
       "0      10001      80.954924 -0.000840              NaN  \n",
       "0       5001      86.349071 -0.000065              1.0  \n",
       "0       4001      88.784448  0.000230              1.0  \n",
       "0       4001      88.987396  0.000125              1.0  \n",
       "0       6001      89.035463  0.000170              1.0  \n",
       "0       4001      89.099551  0.000265              1.0  \n",
       "0       4001      89.131596  0.000040              1.0  \n",
       "0       3001      89.158300  0.000180              1.0  \n",
       "0       8001      89.190344 -0.000270              NaN  \n",
       "0       6001      89.323862  0.000120              1.0  \n",
       "0       5001      89.334544  0.000200              1.0  \n",
       "0       5001      89.345225  0.000150              1.0  \n",
       "0       3001      89.366588  0.000270              1.0  \n",
       "0       6001      89.398633 -0.000030              1.0  \n",
       "0       5001      89.419996  0.000315              1.0  \n",
       "0       3001      89.430677  0.000330              1.0  \n",
       "0       5001      89.500107  0.000015              1.0  \n",
       "0       5001      89.532151  0.000090              1.0  \n",
       "0      10001      89.542833  0.000155              0.9  \n",
       "0       9001      89.590899 -0.000040              1.0  \n",
       "0       6001      89.692373  0.000385              1.0  \n",
       "0       9001      89.692373 -0.000030              1.0  \n",
       "0       3001      89.708396  0.000015              1.0  \n",
       "0       3001      89.809870 -0.000050              0.9  \n",
       "0      20001      89.809870 -0.000510              1.0  \n",
       "0      12001      89.911344 -0.000070              1.0  \n",
       "0       8001      90.018159 -0.000025              NaN  \n",
       "0      10001      90.039521 -0.000270              0.9  \n",
       "0       3001      90.087588  0.000530              0.9  \n",
       "0      10001      90.092929  0.000305              0.9  \n",
       "0       3001      90.098270  0.000065              0.9  \n",
       "0       4001      90.140996 -0.000605              0.9  \n",
       "0       4001      90.140996  0.000190              0.9  \n",
       "0      10001      90.146336  0.000030              0.9  \n",
       "0      10001      90.221107  0.000300              0.9  \n",
       "0      20001      90.322581 -0.000285              1.0  \n",
       "0       4001      90.343944 -0.000330              0.9  \n",
       "0      20001      90.440077 -0.000245              1.0  \n",
       "0      20001      90.482803 -0.000275              NaN  \n",
       "0      15001      90.488144  0.000065              1.0  \n",
       "0      12001      90.493484  0.000295              1.0  \n",
       "0      10001      90.509507 -0.000245              0.9  \n",
       "0       8001      90.610981 -0.000065              0.9  \n",
       "0      10001      90.637684  0.000020              0.9  \n",
       "0       8001      90.643025  0.000175              0.9  \n",
       "0      12001      90.659047 -0.000035              1.0  \n",
       "0       8001      90.739158  0.000135              0.9  \n",
       "0       8001      90.765862  0.000300              0.9  "
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colabVersion": "0.3.2",
  "colab_default_view": {},
  "colab_views": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
